import numpy as np
import pandas as pd
import keras
from keras.models import Model
from keras.layers import Embedding, Input, Dense
from keras.layers import LSTM, GRU
from keras.optimizers import Adam
class Models:
    def usingLSTM(input_seq, onehot_output_seq, embedding_matrix, max_seq_len, unit_dim, num_words, batch_size):
        '''
        This function takes in input sequence, one hot output sequence, embedding matrix, maximum sequence length, unit dimension of LSTM units,
        maximum number of words and batch size as input and returns model, embedding layer, lstm layer, dense layer, initial hidden and cell state values
        as output.

        Parameters:
        input_seq (list) : A list of input sequences
        onehot_output_seq (list) : A list of one hot output sequences
        embedding_matrix (numpy 2d array) : A matrix of word vectors
        max_seq_len (int) : The maximum length of input sequence
        unit_dim (int) : The dimension of LSTM units
        num_words (int) : The maximum number of words
        batch_size (int) : The size of each batch

        Returns:
        model (Keras Model) : A keras model trained on input_seq and onehot_output_seq
        embedding_layer (Keras Layer) : The embedding layer in the trained model
        lstm_layer (Keras Layer) : The LSTM layer from the above model
        dense_layer (Keras Layer) : The Dense Layer from the above model
        initial_h (Keras Layer) : The initial hidden state input layer
        initial_c (Keras Layer) : The initial cell state input layer
        '''
        embedding_layer = Embedding(embedding_matrix.shape[0],
                                    embedding_matrix.shape[1],
                                    weights = [embedding_matrix],
                                    trainable = False)

        input =  Input(shape = (max_seq_len, ))
        x = embedding_layer(input)

        initial_h = Input(shape = (unit_dim,))
        initial_c = Input(shape = (unit_dim,))

        lstm_layer = LSTM(unit_dim, return_sequences = True, return_state = True)
        x, _, _ = lstm_layer(x, initial_state = [initial_h, initial_c])

        dense_layer = Dense(num_words, activation = 'softmax')
        output = dense_layer(x)

        model = Model([input, initial_h, initial_c], output)
        model.compile(loss = 'categorical_crossentropy',
                    optimizer = Adam(lr = 1e-3, decay = 1e-6),
                    metrics = ['accuracy'])

        hidden = np.zeros((len(input_seq), unit_dim))
        cell = np.zeros((len(input_seq), unit_dim))
        model.fit([input_seq, hidden, cell],
                onehot_output_seq,
                epochs = 100,
                batch_size = batch_size,
                validation_split = 0.1)

        return model, embedding_layer, lstm_layer, dense_layer, initial_h, initial_c

    def samplingModel(embedding_layer, lstm_layer, dense_layer, initial_hidden, initial_cell):
        '''
        This function takes in the embedding layer, lstm layer, dense layer, initial hidden and initial cell from the model trained on input sequence and
        one hot output sequence and returns a model that takes in one word at a time.

        Parameters:
        embedding_layer (Keras Layer) : The embedding layer in the trained model
        lstm_layer (Keras Layer) : The LSTM layer from the above model
        dense_layer (Keras Layer) : The Dense Layer from the above model
        initial_h (Keras Layer) : The initial hidden state input layer
        initial_c (Keras Layer) : The initial cell state input layer

        Returns:
        sampling_model (Keras Model) : A model that uses above pretrained layers but takes one word at a time.
        '''
        input = Input(shape = (1,))
        x = embedding_layer(input)
        x, hidden, cell = lstm_layer(x, initial_state = [initial_hidden, initial_cell])
        output = dense_layer(x)
        sampling_model = Model([input, initial_hidden, initial_cell], [output, hidden, cell])
        return sampling_model

    def sampleFromModel(sampling_model, word2idx, idx2word, max_seq_len, unit_dim):
        '''
        This function takes in the sampling model, word2idx, idx2word, maximum sequence length and unit dimension of LSTM units as input
        and returns the output sequence generated by the sampling model.

        sampling_model (Keras Model) : A model that uses above pretrained layers but takes one word at a time.
        word2idx (dict) : A dictionary containing words in key and their corresponding index in value
        idx2word (dict) : A dictionary containing indices in key and their corresponding word in value
        max_seq_len (int) : The maximum length of input sequence
        unit_dim (int) : The dimension of LSTM units

        Returns:
        output_sentence (str) : A string that contains output sequence generated by the sampling model.
        '''
        output_sentence = []
        sos_idx, eos_idx = word2idx['<sos>'], word2idx['<eos>']
        input = np.array([[sos_idx]])
        hidden = np.zeros((1, unit_dim))
        cell = np.zeros((1, unit_dim))

        for word in range(max_seq_len):
            output, hidden, cell = sampling_model.predict([input, hidden, cell])
            probs = output[0, 0]
            if np.argmax(probs) == 0:
                print("MAX")
            probs[0] = 0
            probs /= probs.sum()
            idx = np.random.choice(len(probs), p = probs)
            if idx == eos_idx:
                break

            output_sentence.append(idx2word.get(idx))
            input[0, 0] = idx

        return ' '.join(output_sentence)
